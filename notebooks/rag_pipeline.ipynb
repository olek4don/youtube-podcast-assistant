{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import json\n",
    "import requests\n",
    "from typing import Dict, Any, Optional\n",
    "from abc import ABC, abstractmethod\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "client = OpenAI()\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set the OpenAI API key environment variable\n",
    "# os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "###############################################################\n",
    "\n",
    "\n",
    "class LLMClient(ABC):\n",
    "    @abstractmethod\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        \"\"\"\n",
    "        Abstract method to generate a response based on the given prompt.\n",
    "\n",
    "        Args:\n",
    "            prompt (str): The input prompt to generate a response for.\n",
    "\n",
    "        Returns:\n",
    "            str: The generated response.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class OpenAIClient(LLMClient):\n",
    "    def __init__(self, api_key: str, model: str = \"gpt-3.5-turbo\"):\n",
    "        self.client = OpenAI(api_key=api_key)\n",
    "        self.model = model\n",
    "\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ],\n",
    "                max_tokens=256,\n",
    "                temperature=0.3,\n",
    "            )\n",
    "            return response.choices[0].message.content.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Error in OpenAI API call: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "\n",
    "class OllamaClient(LLMClient):\n",
    "\n",
    "    def __init__(self, model: str = \"llama2:7b\"):\n",
    "        self.model = model\n",
    "        self.api_url = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        payload = {\n",
    "            \"model\": self.model,\n",
    "            \"prompt\": f\"[INST] {prompt} [/INST]\",\n",
    "            \"stream\": False,\n",
    "        }\n",
    "        try:\n",
    "            response = requests.post(self.api_url, json=payload)\n",
    "            response.raise_for_status()  # This will raise an HTTPError for bad responses\n",
    "            return response.json()[\"response\"]\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error making request to Ollama API: {e}\")\n",
    "            print(f\"Response content: {response.text if response else 'No response'}\")\n",
    "            raise Exception(f\"Ollama API error: {e}\")\n",
    "\n",
    "\n",
    "# class HuggingFaceClient(LLMClient):\n",
    "\n",
    "#     def __init__(self, model: str = \"meta-llama/Meta-Llama-3-8B\"):\n",
    "\n",
    "#         from transformers import pipeline\n",
    "\n",
    "#         self.generator = pipeline(\"text-generation\", model=model)\n",
    "\n",
    "#     def generate(self, prompt: str) -> str:\n",
    "#         response = self.generator(prompt, max_length=100, num_return_sequences=1)\n",
    "#         return response[0][\"generated_text\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# def build_prompt(context: str, template: Optional[str] = None) -> str:\n",
    "#     default_template = \"\"\"\n",
    "#     Generate an answer based on the following context and question:\n",
    "#     {context}\n",
    "#     Respond with a JSON object containing \"question\" and \"answer\" keys.\n",
    "#     If there's not enough information, use \"NA\" as the answer.\n",
    "#     \"\"\".strip()\n",
    "\n",
    "#     prompt_template = template or default_template\n",
    "#     return prompt_template.format(context=context).strip()\n",
    "\n",
    "\n",
    "def build_prompt(\n",
    "    query: str, search_results: List[Dict], template: Optional[str] = None\n",
    "):\n",
    "    prompt_template = \"\"\"\n",
    "    Generate an answer based on the following context:\n",
    "    CONTEXT:{context}\n",
    "    Respond with a JSON object containing \"answer\" key.\n",
    "    If there's not enough information, use \"NA\" as the answer.\n",
    "    \"\"\".strip()\n",
    "\n",
    "    context = \"\"\n",
    "\n",
    "    for doc in search_results:\n",
    "        context = context + f\"text: {doc['text']}\\nquestion: {doc['question']}\\n\\n\"\n",
    "\n",
    "    prompt = template or prompt_template.format(question=query, context=context).strip()\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def rag(context: str, llm_client: LLMClient, template: Optional[str] = None) -> str:\n",
    "    prompt = build_prompt(context, template)\n",
    "    response = llm_client.generate(prompt)\n",
    "    # return json.loads(response)\n",
    "    return response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Using Ollama\n",
    "ollama_client = OllamaClient()\n",
    "# try:\n",
    "#     result_ollama = rag(\"Sample context about AI\", ollama_client)\n",
    "#     print(\"Ollama result:\", result_ollama)\n",
    "# except Exception as e:\n",
    "#     print(f\"An error occurred: {e}\")\n",
    "#     # Additional debugging information\n",
    "#     print(\"Available Ollama models:\")\n",
    "#     os.system(\"ollama list\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "from typing import List, Dict\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def generate_answers(dataset: List[Dict], llm_client: LLMClient) -> List[Dict]:\n",
    "    def process_item(item):\n",
    "        context = item[\"text\"]\n",
    "        question = item[\"question\"]\n",
    "        template = f\"Context: {context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
    "        answer = rag(context, llm_client, template)\n",
    "        item[\"answer\"] = answer\n",
    "        return item\n",
    "\n",
    "    total_items = len(dataset)\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        results = list(\n",
    "            tqdm(\n",
    "                executor.map(process_item, dataset),\n",
    "                total=total_items,\n",
    "                desc=\"Generating Answers\",\n",
    "                unit=\"item\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# def generate_answers(dataset: List[Dict], llm_client: LLMClient) -> List[Dict]:\n",
    "#     def process_item(item):\n",
    "#         context = item[\"text\"]\n",
    "#         question = item[\"question\"]\n",
    "#         template = f\"Generate answer based on following context and question: Context: {context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
    "#         answer = rag(context, llm_client, template)\n",
    "#         item[\"answer\"] = answer\n",
    "#         return item\n",
    "\n",
    "#     with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "#         results = list(executor.map(process_item, dataset))\n",
    "\n",
    "#     return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "with open(\"../data/external/transcript_qna_parsed.json\", \"r\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import minsearch\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json(\"../data/external/transcript_qna_parsed.json\")\n",
    "# print(df)\n",
    "df.to_csv(\"../data/external/transcript_qna_parsed.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "processed_dataset = generate_answers(data, ollama_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Using OpenAI\n",
    "openai_client = OpenAIClient(api_key=os.getenv(\"OPENAI_API_KEY\"), model=\"gpt-3.5-turbo\")\n",
    "result_openai = rag(\"Sample context about AI\", openai_client)\n",
    "print(\"OpenAI result:\", result_openai)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
